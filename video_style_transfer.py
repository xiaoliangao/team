"""
è§†é¢‘é£æ ¼è¿ç§» - æ”¯æŒé€å¸§å¤„ç†ã€å¸§é—´ä¸€è‡´æ€§ä¼˜åŒ–å’Œæ–­ç‚¹ç»­ä¼ 
"""

import os
import json
import time
from pathlib import Path
from typing import Optional, Callable, Dict, List
import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F
from tqdm import tqdm

from neural_style_transfer import NeuralStyleTransfer


class VideoStyleTransfer:
    """è§†é¢‘é£æ ¼è¿ç§»ç±»"""
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        """
        åˆå§‹åŒ–è§†é¢‘é£æ ¼è¿ç§»
        
        Args:
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.nst = NeuralStyleTransfer(device=device)
        
    def extract_frames(self, video_path: str, output_dir: str, 
                       max_frames: Optional[int] = None) -> Dict:
        """
        ä»è§†é¢‘ä¸­æå–å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: å¸§è¾“å‡ºç›®å½•
            max_frames: æœ€å¤§æå–å¸§æ•°ï¼ˆNoneè¡¨ç¤ºæå–æ‰€æœ‰å¸§ï¼‰
            
        Returns:
            è§†é¢‘ä¿¡æ¯å­—å…¸
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        frames_dir = os.path.join(output_dir, 'frames')
        os.makedirs(frames_dir, exist_ok=True)
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘å±æ€§
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0 or fps is None:
            fps = 30.0  # é»˜è®¤å¸§ç‡
        fps = float(fps)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # é™åˆ¶å¸§æ•°
        frames_to_extract = min(total_frames, max_frames) if max_frames else total_frames
        
        print(f"è§†é¢‘ä¿¡æ¯:")
        print(f"  åˆ†è¾¨ç‡: {width}x{height}")
        print(f"  å¸§ç‡: {fps} FPS")
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  æå–å¸§æ•°: {frames_to_extract}")
        
        # æå–å¸§
        frame_paths = []
        for i in tqdm(range(frames_to_extract), desc="æå–è§†é¢‘å¸§"):
            ret, frame = cap.read()
            if not ret:
                break
            
            # ä¿å­˜å¸§
            frame_path = os.path.join(frames_dir, f'frame_{i:06d}.jpg')
            cv2.imwrite(frame_path, frame)
            frame_paths.append(frame_path)
        
        cap.release()
        
        # ä¿å­˜è§†é¢‘ä¿¡æ¯
        video_info = {
            'fps': fps,
            'width': width,
            'height': height,
            'total_frames': total_frames,
            'extracted_frames': len(frame_paths),
            'frame_paths': frame_paths
        }
        
        info_path = os.path.join(output_dir, 'video_info.json')
        with open(info_path, 'w') as f:
            json.dump(video_info, f, indent=2)
        
        return video_info
    
    def process_frame_with_consistency(self, 
                                      current_frame: torch.Tensor,
                                      style_img: torch.Tensor,
                                      prev_output: Optional[torch.Tensor] = None,
                                      num_steps: int = 200,
                                      style_weight: float = 1e6,
                                      content_weight: float = 1.0,
                                      temporal_weight: float = 1e4) -> torch.Tensor:
        """
        å¤„ç†å•å¸§ï¼Œå¸¦å¸§é—´ä¸€è‡´æ€§ä¼˜åŒ–
        
        Args:
            current_frame: å½“å‰å¸§
            style_img: é£æ ¼å›¾åƒ
            prev_output: ä¸Šä¸€å¸§çš„è¾“å‡ºï¼ˆç”¨äºä¿æŒä¸€è‡´æ€§ï¼‰
            num_steps: ä¼˜åŒ–æ­¥æ•°
            style_weight: é£æ ¼æƒé‡
            content_weight: å†…å®¹æƒé‡
            temporal_weight: æ—¶é—´ä¸€è‡´æ€§æƒé‡
            
        Returns:
            å¤„ç†åçš„å¸§
        """
        # å¦‚æœæœ‰å‰ä¸€å¸§ï¼Œä½¿ç”¨å®ƒä½œä¸ºåˆå§‹åŒ–ï¼ˆæä¾›æ›´å¥½çš„ä¸€è‡´æ€§ï¼‰
        if prev_output is not None:
            input_img = prev_output.clone()
        else:
            input_img = current_frame.clone()
        
        # æ„å»ºæ¨¡å‹
        model, style_losses, content_losses = self.nst.get_style_model_and_losses(
            style_img, current_frame)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.LBFGS([input_img.requires_grad_()])
        
        run = [0]
        while run[0] <= num_steps:
            def closure():
                with torch.no_grad():
                    input_img.clamp_(0, 1)
                
                optimizer.zero_grad()
                model(input_img)
                
                # è®¡ç®—æŸå¤±
                style_score = sum(sl.loss for sl in style_losses) * style_weight
                content_score = sum(cl.loss for cl in content_losses) * content_weight
                
                # æ·»åŠ æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼ˆå¦‚æœæœ‰å‰ä¸€å¸§ï¼‰
                temporal_score = 0
                if prev_output is not None:
                    temporal_score = F.mse_loss(input_img, prev_output) * temporal_weight
                
                loss = style_score + content_score + temporal_score
                loss.backward()
                
                run[0] += 1
                return loss
            
            optimizer.step(closure)
        
        with torch.no_grad():
            input_img.clamp_(0, 1)
        
        return input_img
    
    def process_video(self, 
                     video_path: str,
                     style_image_path: str,
                     output_path: str,
                     num_steps: int = 200,
                     style_weight: float = 1e6,
                     content_weight: float = 1.0,
                     temporal_weight: float = 1e4,
                     image_size: int = 512,
                     max_frames: Optional[int] = None,
                     use_consistency: bool = True,
                     progress_callback: Optional[Callable] = None) -> Dict:
        """
        å¤„ç†æ•´ä¸ªè§†é¢‘
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            style_image_path: é£æ ¼å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            num_steps: æ¯å¸§çš„ä¼˜åŒ–æ­¥æ•°
            style_weight: é£æ ¼æƒé‡
            content_weight: å†…å®¹æƒé‡
            temporal_weight: æ—¶é—´ä¸€è‡´æ€§æƒé‡
            image_size: å¤„ç†å›¾åƒå¤§å°
            max_frames: æœ€å¤§å¤„ç†å¸§æ•°
            use_consistency: æ˜¯å¦ä½¿ç”¨å¸§é—´ä¸€è‡´æ€§ä¼˜åŒ–
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            å¤„ç†ä¿¡æ¯å­—å…¸
        """
        start_time = time.time()
        
        # è®¾ç½®NSTå›¾åƒå¤§å°
        self.nst.imsize = image_size
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        work_dir = os.path.join(os.path.dirname(output_path), 
                               f'work_{Path(output_path).stem}')
        os.makedirs(work_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹ç»­ä¼ æ•°æ®
        checkpoint_path = os.path.join(work_dir, 'checkpoint.json')
        checkpoint = self._load_checkpoint(checkpoint_path)
        
        # æå–å¸§ï¼ˆå¦‚æœå°šæœªæå–ï¼‰
        if checkpoint and 'video_info' in checkpoint:
            print("ä»æ–­ç‚¹æ¢å¤...")
            video_info = checkpoint['video_info']
            start_frame = checkpoint.get('last_processed_frame', 0) + 1
        else:
            print("æå–è§†é¢‘å¸§...")
            video_info = self.extract_frames(video_path, work_dir, max_frames)
            start_frame = 0
        
        # åŠ è½½é£æ ¼å›¾åƒ
        print("åŠ è½½é£æ ¼å›¾åƒ...")
        style_img = self.nst.load_image(style_image_path)
        
        # åˆ›å»ºè¾“å‡ºå¸§ç›®å½•
        output_frames_dir = os.path.join(work_dir, 'styled_frames')
        os.makedirs(output_frames_dir, exist_ok=True)
        
        # å¤„ç†æ¯ä¸€å¸§
        frame_paths = video_info['frame_paths']
        total_frames = len(frame_paths)
        
        prev_output = None
        processed_frames = []
        
        print(f"\nå¼€å§‹å¤„ç†è§†é¢‘ï¼ˆä»ç¬¬ {start_frame} å¸§å¼€å§‹ï¼‰...")
        
        for i in range(start_frame, total_frames):
            frame_start_time = time.time()
            
            # åŠ è½½å½“å‰å¸§
            current_frame = self.nst.load_image(frame_paths[i])
            
            # å¤„ç†å¸§
            if use_consistency and prev_output is not None:
                output = self.process_frame_with_consistency(
                    current_frame, style_img, prev_output,
                    num_steps, style_weight, content_weight, temporal_weight
                )
            else:
                output = self.nst.run_style_transfer(
                    current_frame, style_img, num_steps,
                    style_weight, content_weight
                )
            
            # ä¿å­˜è¾“å‡ºå¸§
            output_frame_path = os.path.join(output_frames_dir, f'styled_{i:06d}.jpg')
            output_pil = self.nst.show_image(output)
            output_pil.save(output_frame_path, quality=95)
            processed_frames.append(output_frame_path)
            
            # ä¿å­˜ä¸ºä¸‹ä¸€å¸§çš„å‚è€ƒ
            prev_output = output.detach()
            
            # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
            frame_time = time.time() - frame_start_time
            progress = (i + 1) / total_frames
            remaining_frames = total_frames - (i + 1)
            eta = remaining_frames * frame_time
            
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(progress, f"å¤„ç†ç¬¬ {i+1}/{total_frames} å¸§")
            
            print(f"å¸§ {i+1}/{total_frames} å®Œæˆ "
                  f"({progress*100:.1f}%) - "
                  f"è€—æ—¶: {frame_time:.1f}s - "
                  f"é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (i + 1) % 10 == 0:  # æ¯10å¸§ä¿å­˜ä¸€æ¬¡
                self._save_checkpoint(checkpoint_path, {
                    'video_info': video_info,
                    'last_processed_frame': i,
                    'processed_frames': processed_frames,
                    'params': {
                        'num_steps': num_steps,
                        'style_weight': style_weight,
                        'content_weight': content_weight,
                        'temporal_weight': temporal_weight,
                        'image_size': image_size
                    }
                })
        
        # åˆæˆè§†é¢‘
        print("\nåˆæˆæœ€ç»ˆè§†é¢‘...")
        self._create_video_from_frames(
            processed_frames,
            output_path,
            float(video_info['fps']),
            (image_size, image_size)
        )
        
        total_time = time.time() - start_time
        
        # æ¸…ç†æ£€æŸ¥ç‚¹
        if os.path.exists(checkpoint_path):
            os.remove(checkpoint_path)
        
        # è¿”å›å¤„ç†ä¿¡æ¯
        return {
            'input_video': video_path,
            'output_video': output_path,
            'style_image': style_image_path,
            'total_frames': total_frames,
            'fps': video_info['fps'],
            'resolution': f"{image_size}x{image_size}",
            'processing_time': total_time,
            'time_per_frame': total_time / total_frames,
            'params': {
                'num_steps': num_steps,
                'style_weight': style_weight,
                'temporal_weight': temporal_weight,
                'use_consistency': use_consistency
            }
        }
    
    def _create_video_from_frames(self, frame_paths: List[str], 
                                 output_path: str, fps: float,
                                 frame_size: tuple):
        """
        ä»å¸§åºåˆ—åˆ›å»ºè§†é¢‘
        
        Args:
            frame_paths: å¸§æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡
            frame_size: å¸§å¤§å° (width, height)
        """
        if not frame_paths:
            raise ValueError("æ²¡æœ‰å¸§å¯ä»¥åˆæˆè§†é¢‘")
        
        # ç¡®ä¿fpsæ˜¯æœ‰æ•ˆå€¼
        fps = float(fps)
        if fps <= 0:
            fps = 30.0
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶æ˜¯.mp4æ ¼å¼
        if not output_path.endswith('.mp4'):
            output_path = os.path.splitext(output_path)[0] + '.mp4'
        
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨FFmpegï¼ˆæ›´å¯é ï¼‰
        print("å°è¯•ä½¿ç”¨FFmpegåˆ›å»ºè§†é¢‘...")
        success = self._create_video_with_ffmpeg(frame_paths, output_path, fps, frame_size)
        
        if success:
            return
        
        # å¦‚æœFFmpegå¤±è´¥ï¼Œå°è¯•OpenCV
        print("\nâš ï¸ FFmpegä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨OpenCVç¼–ç å™¨...")
        
        # ä½¿ç”¨ H.264 ç¼–ç å™¨ï¼Œå…¼å®¹æ€§æ›´å¥½
        # å°è¯•å¤šä¸ªç¼–ç å™¨ä»¥ç¡®ä¿å…¼å®¹æ€§
        codecs = [
            ('mp4v', '.mp4'),  # MPEG-4 (macOSå…¼å®¹æ€§æ›´å¥½)
            ('avc1', '.mp4'),  # H.264
            ('XVID', '.avi'),  # Xvid
        ]
        
        success = False
        out = None
        
        for codec, ext in codecs:
            try:
                # è°ƒæ•´è¾“å‡ºæ–‡ä»¶æ‰©å±•å
                if not output_path.endswith(ext):
                    temp_output_path = os.path.splitext(output_path)[0] + ext
                else:
                    temp_output_path = output_path
                
                fourcc = cv2.VideoWriter_fourcc(*codec)
                out = cv2.VideoWriter(temp_output_path, fourcc, fps, frame_size)
                
                if out.isOpened():
                    print(f"ä½¿ç”¨ç¼–ç å™¨: {codec}")
                    
                    frames_written = 0
                    for frame_path in tqdm(frame_paths, desc="åˆæˆè§†é¢‘"):
                        # è¯»å–å›¾åƒ
                        frame = cv2.imread(frame_path)
                        if frame is not None:
                            # è°ƒæ•´å¤§å°ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            if frame.shape[1] != frame_size[0] or frame.shape[0] != frame_size[1]:
                                frame = cv2.resize(frame, frame_size, interpolation=cv2.INTER_LANCZOS4)
                            
                            # ç¡®ä¿å›¾åƒæ˜¯æ­£ç¡®çš„æ ¼å¼ (BGR, uint8)
                            if frame.dtype != np.uint8:
                                frame = (frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame.astype(np.uint8)
                            
                            # ç¡®ä¿æ˜¯3é€šé“BGRå›¾åƒ
                            if len(frame.shape) == 2:
                                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
                            elif frame.shape[2] == 4:
                                frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)
                            
                            out.write(frame)
                            frames_written += 1
                    
                    out.release()
                    
                    # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
                    if os.path.exists(temp_output_path) and os.path.getsize(temp_output_path) > 0:
                        print(f"âœ… è§†é¢‘å·²ä¿å­˜: {temp_output_path}")
                        print(f"   å¸§æ•°: {frames_written}, æ–‡ä»¶å¤§å°: {os.path.getsize(temp_output_path)/1024/1024:.2f} MB")
                        
                        # å¦‚æœä¸´æ—¶è·¯å¾„ä¸ç›®æ ‡è·¯å¾„ä¸åŒï¼Œé‡å‘½å
                        if temp_output_path != output_path:
                            if os.path.exists(output_path):
                                os.remove(output_path)
                            os.rename(temp_output_path, output_path)
                            print(f"   å·²é‡å‘½åä¸º: {output_path}")
                        
                        success = True
                        break
                    else:
                        print(f"âš ï¸ ç¼–ç å™¨ {codec} åˆ›å»ºçš„æ–‡ä»¶æ— æ•ˆï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                        if out:
                            out.release()
                        if os.path.exists(temp_output_path):
                            os.remove(temp_output_path)
                else:
                    print(f"âš ï¸ æ— æ³•ä½¿ç”¨ç¼–ç å™¨ {codec}ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                    
            except Exception as e:
                print(f"âš ï¸ ç¼–ç å™¨ {codec} å¤±è´¥: {str(e)}")
                if out:
                    out.release()
                continue
        
        if not success:
            raise RuntimeError(
                "æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶ã€‚è¯·ç¡®ä¿ç³»ç»Ÿå·²å®‰è£…å¿…è¦çš„è§†é¢‘ç¼–è§£ç å™¨ã€‚\n"
                "åœ¨ macOS ä¸Šï¼Œè¯·é€šè¿‡ Homebrew å®‰è£…: brew install ffmpeg\n"
                "æˆ–è€…å®‰è£…å®Œæ•´çš„opencv: pip install opencv-contrib-python"
            )
    
    def _create_video_with_ffmpeg(self, frame_paths: List[str], 
                                   output_path: str, fps: float,
                                   frame_size: tuple) -> bool:
        """
        ä½¿ç”¨FFmpegåˆ›å»ºè§†é¢‘(æ¨èæ–¹æ¡ˆ)
        
        Args:
            frame_paths: å¸§æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡
            frame_size: å¸§å¤§å° (width, height)
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            import subprocess
            import shutil
            
            # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
            if not shutil.which('ffmpeg'):
                print("âš ï¸ FFmpegæœªå®‰è£…")
                print("   åœ¨macOSä¸Šå®‰è£…: brew install ffmpeg")
                print("   åœ¨Ubuntuä¸Šå®‰è£…: sudo apt-get install ffmpeg")
                return False
            
            # æ£€æŸ¥å¸§æ–‡ä»¶çš„å®é™…å°ºå¯¸
            first_frame = cv2.imread(frame_paths[0])
            if first_frame is None:
                print(f"âš ï¸ æ— æ³•è¯»å–ç¬¬ä¸€å¸§: {frame_paths[0]}")
                return False
            
            actual_height, actual_width = first_frame.shape[:2]
            print(f"   æ£€æµ‹åˆ°å¸§å°ºå¯¸: {actual_width}x{actual_height}")
            
            # ä½¿ç”¨å®é™…å°ºå¯¸æˆ–æŒ‡å®šå°ºå¯¸
            if (actual_width, actual_height) != frame_size:
                print(f"   å°†è°ƒæ•´ä¸º: {frame_size[0]}x{frame_size[1]}")
                scale_filter = f'scale={frame_size[0]}:{frame_size[1]}'
            else:
                scale_filter = None
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å¹¶å¤åˆ¶/é“¾æ¥å¸§æ–‡ä»¶
            work_dir = os.path.dirname(frame_paths[0])
            
            # åˆ›å»ºç¬¦å·é“¾æ¥ä»¥ç¬¦åˆffmpegçš„å‘½åè¦æ±‚
            print("   å‡†å¤‡å¸§æ–‡ä»¶...")
            link_paths = []
            for i, frame_path in enumerate(frame_paths):
                link_path = os.path.join(work_dir, f'ffmpeg_frame_{i:06d}.jpg')
                if os.path.exists(link_path):
                    os.remove(link_path)
                try:
                    os.symlink(os.path.abspath(frame_path), link_path)
                    link_paths.append(link_path)
                except:
                    # å¦‚æœç¬¦å·é“¾æ¥å¤±è´¥,å°è¯•å¤åˆ¶
                    shutil.copy2(frame_path, link_path)
                    link_paths.append(link_path)
            
            # ä½¿ç”¨ffmpegå‘½ä»¤
            input_pattern = os.path.join(work_dir, 'ffmpeg_frame_%06d.jpg')
            cmd = [
                'ffmpeg',
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                '-framerate', str(fps),
                '-i', input_pattern,
                '-c:v', 'libx264',  # H.264ç¼–ç å™¨
                '-preset', 'medium',  # ç¼–ç é€Ÿåº¦ï¼ˆfast/medium/slowï¼‰
                '-crf', '23',  # è´¨é‡ (18-28æ¨èï¼Œè¶Šå°è´¨é‡è¶Šé«˜ä½†æ–‡ä»¶è¶Šå¤§)
                '-pix_fmt', 'yuv420p',  # åƒç´ æ ¼å¼(yuv420på…¼å®¹æ€§æœ€å¥½)
                '-movflags', '+faststart',  # ä¼˜åŒ–ç½‘ç»œæ’­æ”¾
            ]
            
            # æ·»åŠ ç¼©æ”¾æ»¤é•œï¼ˆå¦‚æœéœ€è¦ï¼‰
            if scale_filter:
                cmd.extend(['-vf', scale_filter])
            
            cmd.append(output_path)
            
            print(f"   æ‰§è¡ŒFFmpegå‘½ä»¤...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for link_path in link_paths:
                if os.path.exists(link_path):
                    try:
                        os.remove(link_path)
                    except:
                        pass
            
            if result.returncode == 0 and os.path.exists(output_path):
                file_size = os.path.getsize(output_path) / 1024 / 1024
                print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path}")
                print(f"   å¸§æ•°: {len(frame_paths)}, æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
                
                # éªŒè¯è§†é¢‘å¯ä»¥è¢«è¯»å–
                cap = cv2.VideoCapture(output_path)
                if cap.isOpened():
                    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    cap.release()
                    print(f"   éªŒè¯æˆåŠŸ: è§†é¢‘åŒ…å« {frame_count} å¸§")
                    return True
                else:
                    print(f"âš ï¸ è§†é¢‘æ–‡ä»¶åˆ›å»ºæˆåŠŸä½†æ— æ³•æ‰“å¼€ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜")
                    return False
            else:
                print(f"âš ï¸ FFmpegæ‰§è¡Œå¤±è´¥")
                if result.stderr:
                    # åªæ˜¾ç¤ºæœ€åå‡ è¡Œé”™è¯¯ä¿¡æ¯
                    error_lines = result.stderr.strip().split('\n')
                    print("   é”™è¯¯ä¿¡æ¯:")
                    for line in error_lines[-5:]:
                        print(f"   {line}")
                return False
                
        except Exception as e:
            print(f"âš ï¸ FFmpegæ–¹æ¡ˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def _save_checkpoint(self, checkpoint_path: str, data: Dict):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        with open(checkpoint_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    def _load_checkpoint(self, checkpoint_path: str) -> Optional[Dict]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            try:
                with open(checkpoint_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return None
        return None
    
    def create_preview_gif(self, video_path: str, output_path: str, 
                          num_frames: int = 10, fps: int = 5, 
                          max_width: int = 480):
        """
        ä»è§†é¢‘åˆ›å»ºé¢„è§ˆ GIF
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡º GIF è·¯å¾„
            num_frames: GIF å¸§æ•°
            fps: GIF å¸§ç‡
            max_width: æœ€å¤§å®½åº¦ï¼ˆç”¨äºå‹ç¼©ï¼‰
        """
        try:
            from PIL import Image
            
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames == 0:
                print("âš ï¸ è§†é¢‘æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–")
                cap.release()
                return False
            
            # å‡åŒ€é‡‡æ ·å¸§
            frame_indices = np.linspace(0, total_frames-1, min(num_frames, total_frames), dtype=int)
            
            frames = []
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # è½¬æ¢é¢œè‰²ç©ºé—´
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # è°ƒæ•´å¤§å°ä»¥å‡å°æ–‡ä»¶
                    h, w = frame.shape[:2]
                    if w > max_width:
                        scale = max_width / w
                        new_w = max_width
                        new_h = int(h * scale)
                        frame = cv2.resize(frame, (new_w, new_h))
                    
                    frames.append(Image.fromarray(frame))
            
            cap.release()
            
            if frames:
                # ä¿å­˜ä¸º GIF
                duration = int(1000 / fps)  # æ¯«ç§’
                frames[0].save(
                    output_path,
                    save_all=True,
                    append_images=frames[1:],
                    duration=duration,
                    loop=0,
                    optimize=True
                )
                print(f"âœ… é¢„è§ˆ GIF å·²ä¿å­˜: {output_path}")
                return True
            else:
                print("âš ï¸ æ— æ³•æå–è§†é¢‘å¸§")
                return False
                
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºé¢„è§ˆ GIF å¤±è´¥: {str(e)}")
            return False


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================
if __name__ == "__main__":
    import sys
    import argparse
    from datetime import datetime
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # è®¾ç½®é»˜è®¤è·¯å¾„
    default_content_dir = os.path.join(base_dir, 'data', 'content')
    default_style_dir = os.path.join(base_dir, 'data', 'style')
    default_output_dir = os.path.join(base_dir, 'data', 'outputs')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(default_output_dir, exist_ok=True)
    
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(
        description='è§†é¢‘é£æ ¼è¿ç§» - å°†è‰ºæœ¯é£æ ¼åº”ç”¨åˆ°è§†é¢‘ä¸­',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆè‡ªåŠ¨åœ¨ data/ ç›®å½•ä¸­æŸ¥æ‰¾ï¼‰
  python video_style_transfer.py my_video.mp4 starry_night.jpg
  
  # ä½¿ç”¨å®Œæ•´è·¯å¾„
  python video_style_transfer.py data/content/video.mp4 data/style/style.jpg
  
  # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„å’Œå‚æ•°
  python video_style_transfer.py video.mp4 style.jpg -o my_output.mp4 -s 200 -f 200
  
  # è°ƒæ•´å›¾åƒå¤§å°å’Œå…³é—­å¸§é—´ä¸€è‡´æ€§
  python video_style_transfer.py video.mp4 style.jpg --size 1024 --no-consistency
        """)
    
    parser.add_argument('video', type=str,
                       help='è¾“å…¥è§†é¢‘æ–‡ä»¶åï¼ˆåœ¨ data/content/ ä¸­ï¼‰æˆ–å®Œæ•´è·¯å¾„')
    parser.add_argument('style', type=str,
                       help='é£æ ¼å›¾åƒæ–‡ä»¶åï¼ˆåœ¨ data/style/ ä¸­ï¼‰æˆ–å®Œæ•´è·¯å¾„')
    parser.add_argument('-o', '--output', type=str, default=None,
                       help='è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤: data/outputs/styled_TIMESTAMP.mp4ï¼‰')
    parser.add_argument('-s', '--steps', type=int, default=150,
                       help='ä¼˜åŒ–æ­¥æ•°ï¼ˆé»˜è®¤: 150ï¼Œæ›´å¤šæ­¥æ•°è´¨é‡æ›´å¥½ä½†é€Ÿåº¦æ›´æ…¢ï¼‰')
    parser.add_argument('--size', type=int, default=512,
                       help='å¤„ç†å›¾åƒå¤§å°ï¼ˆé»˜è®¤: 512ï¼Œè¶Šå¤§è¶Šæ¸…æ™°ä½†è¶Šæ…¢ï¼‰')
    parser.add_argument('-f', '--max-frames', type=int, default=None,
                       help='æœ€å¤§å¤„ç†å¸§æ•°ï¼ˆé»˜è®¤: æ— é™åˆ¶ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--style-weight', type=float, default=1e6,
                       help='é£æ ¼æƒé‡ï¼ˆé»˜è®¤: 1e6ï¼‰')
    parser.add_argument('--content-weight', type=float, default=1.0,
                       help='å†…å®¹æƒé‡ï¼ˆé»˜è®¤: 1.0ï¼‰')
    parser.add_argument('--temporal-weight', type=float, default=1e4,
                       help='æ—¶é—´ä¸€è‡´æ€§æƒé‡ï¼ˆé»˜è®¤: 1e4ï¼‰')
    parser.add_argument('--no-consistency', action='store_true',
                       help='ç¦ç”¨å¸§é—´ä¸€è‡´æ€§ä¼˜åŒ–ï¼ˆå¤„ç†æ›´å¿«ä½†å¯èƒ½é—ªçƒï¼‰')
    parser.add_argument('--device', type=str, default=None,
                       help='è®¡ç®—è®¾å¤‡: cuda æˆ– cpuï¼ˆé»˜è®¤: è‡ªåŠ¨æ£€æµ‹ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†è§†é¢‘è·¯å¾„
    if os.path.exists(args.video):
        video_path = args.video
    else:
        video_path = os.path.join(default_content_dir, args.video)
        if not os.path.exists(video_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶")
            print(f"   å°è¯•è·¯å¾„: {args.video}")
            print(f"   å°è¯•è·¯å¾„: {video_path}")
            print(f"\nğŸ’¡ æç¤º: è¯·å°†è§†é¢‘æ”¾åœ¨ {default_content_dir} ç›®å½•ä¸­")
            sys.exit(1)
    
    # å¤„ç†é£æ ¼å›¾åƒè·¯å¾„
    if os.path.exists(args.style):
        style_path = args.style
    else:
        style_path = os.path.join(default_style_dir, args.style)
        if not os.path.exists(style_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°é£æ ¼å›¾åƒ")
            print(f"   å°è¯•è·¯å¾„: {args.style}")
            print(f"   å°è¯•è·¯å¾„: {style_path}")
            print(f"\nğŸ’¡ æç¤º: è¯·å°†é£æ ¼å›¾åƒæ”¾åœ¨ {default_style_dir} ç›®å½•ä¸­")
            sys.exit(1)
    
    # å¤„ç†è¾“å‡ºè·¯å¾„
    if args.output:
        if os.path.isabs(args.output):
            output_path = args.output
        else:
            output_path = os.path.join(default_output_dir, args.output)
    else:
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        style_name = os.path.splitext(os.path.basename(style_path))[0]
        output_filename = f"styled_{video_name}_{style_name}_{timestamp}.mp4"
        output_path = os.path.join(default_output_dir, output_filename)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = args.device if args.device else ('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=" * 80)
    print("ğŸ¨ è§†é¢‘é£æ ¼è¿ç§»")
    print("=" * 80)
    print(f"ğŸ“¹ è¾“å…¥è§†é¢‘: {video_path}")
    print(f"ğŸ–¼ï¸  é£æ ¼å›¾åƒ: {style_path}")
    print(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"âš™ï¸  å¤„ç†å‚æ•°:")
    print(f"   - ä¼˜åŒ–æ­¥æ•°: {args.steps}")
    print(f"   - å›¾åƒå¤§å°: {args.size}x{args.size}")
    print(f"   - æœ€å¤§å¸§æ•°: {args.max_frames if args.max_frames else 'æ— é™åˆ¶'}")
    print(f"   - é£æ ¼æƒé‡: {args.style_weight:.0e}")
    print(f"   - å†…å®¹æƒé‡: {args.content_weight}")
    print(f"   - æ—¶é—´ä¸€è‡´æ€§: {'ç¦ç”¨' if args.no_consistency else f'å¯ç”¨ (æƒé‡: {args.temporal_weight:.0e})'}")
    print(f"   - è®¡ç®—è®¾å¤‡: {device.upper()}")
    print("=" * 80)
    print()
    
    # åˆå§‹åŒ–è§†é¢‘é£æ ¼è¿ç§»
    vst = VideoStyleTransfer(device=device)
    
    try:
        # å¤„ç†è§†é¢‘
        result = vst.process_video(
            video_path=video_path,
            style_image_path=style_path,
            output_path=output_path,
            num_steps=args.steps,
            style_weight=args.style_weight,
            content_weight=args.content_weight,
            temporal_weight=args.temporal_weight,
            image_size=args.size,
            max_frames=args.max_frames,
            use_consistency=not args.no_consistency
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("âœ… å¤„ç†å®Œæˆ!")
        print("=" * 80)
        print(f"ğŸ“¹ è¾“å…¥è§†é¢‘: {result['input_video']}")
        print(f"ğŸ¬ è¾“å‡ºè§†é¢‘: {result['output_video']}")
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
        print(f"   - æ€»å¸§æ•°: {result['total_frames']}")
        print(f"   - å¸§ç‡: {result['fps']} FPS")
        print(f"   - åˆ†è¾¨ç‡: {result['resolution']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´:")
        print(f"   - æ€»è€—æ—¶: {result['processing_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   - æ¯å¸§è€—æ—¶: {result['time_per_frame']:.1f} ç§’")
        print(f"ğŸ¨ é£æ ¼å‚æ•°:")
        print(f"   - ä¼˜åŒ–æ­¥æ•°: {result['params']['num_steps']}")
        print(f"   - é£æ ¼æƒé‡: {result['params']['style_weight']:.0e}")
        print(f"   - å¸§é—´ä¸€è‡´æ€§: {'å¯ç”¨' if result['params']['use_consistency'] else 'ç¦ç”¨'}")
        print("=" * 80)
        print(f"\nâœ¨ æ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ç»“æœ: {output_path}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        print("ğŸ’¡ å¤„ç†è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ–­ç‚¹ç»§ç»­")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å¤„ç†å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
